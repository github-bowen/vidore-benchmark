{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal vs. Segmented Model Performance Comparison\n",
    "\n",
    "This notebook compares the performance of models that have both normal and segmented versions in the VIDORE benchmark. The goal is to analyze whether image segmentation improves model performance across different metrics and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set the style for our plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Results\n",
    "\n",
    "First, we'll gather all metrics files from both the normal and segmented directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results directories\n",
    "normal_dir = \"./results/normal/\"\n",
    "segmented_dir = \"./results/segmented/\"\n",
    "\n",
    "# Function to find metrics files in a directory\n",
    "def find_metrics_files(directory):\n",
    "    metrics_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('_metrics.json'):\n",
    "                metrics_files.append(os.path.join(root, file))\n",
    "    return metrics_files\n",
    "\n",
    "# Get all metrics files\n",
    "normal_files = find_metrics_files(normal_dir)\n",
    "segmented_files = find_metrics_files(segmented_dir)\n",
    "\n",
    "print(f\"Found {len(normal_files)} metrics files in normal directory\")\n",
    "print(f\"Found {len(segmented_files)} metrics files in segmented directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Model Information\n",
    "\n",
    "We need to extract structured information about each model, including whether it's normal or segmented, and the base model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify normal and segmented versions of the same model\n",
    "def extract_model_info(model_path):\n",
    "    # Extract model type (normal or segmented) and name from the path\n",
    "    path_parts = model_path.split('/')\n",
    "    model_type = None\n",
    "    \n",
    "    # Find whether this is normal or segmented\n",
    "    for part in path_parts:\n",
    "        if part == 'normal' or part == 'segmented':\n",
    "            model_type = part\n",
    "            break\n",
    "    \n",
    "    # Extract full model name from directory \n",
    "    model_dir = [p for p in path_parts if p != 'results' and p != 'normal' and p != 'segmented' and 'vidore_' not in p][-1]\n",
    "    \n",
    "    # Extract base model name (removing segmentation info if present)\n",
    "    base_model_name = re.sub(r'_seg_\\d+x\\d+(?:_overlap_\\d+)?$', '', model_dir)\n",
    "    \n",
    "    # Extract segmentation pattern if present\n",
    "    seg_pattern = None\n",
    "    if '_seg_' in model_dir:\n",
    "        seg_match = re.search(r'_seg_(\\d+x\\d+)(?:_overlap_(\\d+))?', model_dir)\n",
    "        if seg_match:\n",
    "            grid = seg_match.group(1)\n",
    "            overlap = seg_match.group(2) if seg_match.lastindex >= 2 else None\n",
    "            seg_pattern = f\"{grid}\" + (f\"_overlap_{overlap}\" if overlap else \"\")\n",
    "    \n",
    "    # Extract dataset information from filename\n",
    "    file_name = os.path.basename(model_path)\n",
    "    dataset_match = re.search(r'vidore_([^_]+)', file_name)\n",
    "    dataset = dataset_match.group(1) if dataset_match else None\n",
    "    \n",
    "    return {\n",
    "        'full_path': model_path,\n",
    "        'model_type': model_type,  # 'normal' or 'segmented'\n",
    "        'dir_name': model_dir,\n",
    "        'base_name': base_model_name,\n",
    "        'seg_pattern': seg_pattern,\n",
    "        'dataset': dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files\n",
    "all_files = normal_files + segmented_files\n",
    "model_info_list = [extract_model_info(file) for file in all_files]\n",
    "\n",
    "# Create a DataFrame for easier filtering\n",
    "model_info_df = pd.DataFrame(model_info_list)\n",
    "model_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Models with Both Normal and Segmented Versions\n",
    "\n",
    "We'll identify models that have both normal and segmented versions for the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by base_name and dataset to find matching pairs\n",
    "model_groups = model_info_df.groupby(['base_name', 'dataset'])\n",
    "\n",
    "# Filter to find groups with both normal and segmented versions\n",
    "paired_models = []\n",
    "\n",
    "for (base_name, dataset), group in model_groups:\n",
    "    has_normal = 'normal' in group['model_type'].values\n",
    "    has_segmented = 'segmented' in group['model_type'].values\n",
    "    \n",
    "    if has_normal and has_segmented:\n",
    "        paired_models.append({\n",
    "            'base_name': base_name,\n",
    "            'dataset': dataset,\n",
    "            'normal_path': group[group['model_type'] == 'normal']['full_path'].values[0],\n",
    "            'segmented_paths': group[group['model_type'] == 'segmented']['full_path'].values.tolist(),\n",
    "            'segmentation_patterns': group[group['model_type'] == 'segmented']['seg_pattern'].values.tolist()\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "paired_df = pd.DataFrame(paired_models)\n",
    "\n",
    "print(f\"Found {len(paired_df)} model-dataset combinations with both normal and segmented versions\")\n",
    "paired_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Metrics for Comparison\n",
    "\n",
    "Now we'll load the metrics data for each model pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load metrics from a file path\n",
    "def load_metrics(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metrics = None\n",
    "    if 'metrics' in data:\n",
    "        dataset_key = list(data['metrics'].keys())[0] if data['metrics'] else None\n",
    "        if dataset_key:\n",
    "            metrics = data['metrics'][dataset_key]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Metrics we're interested in comparing\n",
    "metric_types = ['ndcg', 'map', 'recall', 'precision']\n",
    "k_values = [1, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "# Create a DataFrame to store comparison data\n",
    "comparison_data = []\n",
    "\n",
    "for _, row in paired_df.iterrows():\n",
    "    base_name = row['base_name']\n",
    "    dataset = row['dataset']\n",
    "    \n",
    "    # Load normal metrics\n",
    "    normal_metrics = load_metrics(row['normal_path'])\n",
    "    \n",
    "    # For each segmented version\n",
    "    for i, seg_path in enumerate(row['segmented_paths']):\n",
    "        seg_pattern = row['segmentation_patterns'][i] if i < len(row['segmentation_patterns']) else None\n",
    "        segmented_metrics = load_metrics(seg_path)\n",
    "        \n",
    "        # Skip if either metrics couldn't be loaded\n",
    "        if not normal_metrics or not segmented_metrics:\n",
    "            continue\n",
    "        \n",
    "        # Compare all relevant metrics\n",
    "        for metric_type in metric_types:\n",
    "            for k in k_values:\n",
    "                metric_key = f\"{metric_type}_at_{k}\"\n",
    "                \n",
    "                if metric_key in normal_metrics and metric_key in segmented_metrics:\n",
    "                    normal_value = normal_metrics[metric_key]\n",
    "                    segmented_value = segmented_metrics[metric_key]\n",
    "                    \n",
    "                    # Calculate improvement percentage\n",
    "                    if normal_value > 0:\n",
    "                        improvement_pct = (segmented_value - normal_value) / normal_value * 100\n",
    "                    else:\n",
    "                        improvement_pct = np.nan if normal_value == 0 else float('inf')\n",
    "                    \n",
    "                    comparison_data.append({\n",
    "                        'Base Model': base_name,\n",
    "                        'Dataset': dataset,\n",
    "                        'Segmentation Pattern': seg_pattern,\n",
    "                        'Metric': metric_type,\n",
    "                        'k': k,\n",
    "                        'Normal Value': normal_value,\n",
    "                        'Segmented Value': segmented_value,\n",
    "                        'Absolute Difference': segmented_value - normal_value,\n",
    "                        'Improvement (%)': improvement_pct\n",
    "                    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance Comparisons\n",
    "\n",
    "### Overall Distribution of Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of improvement percentages\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(comparison_df['Improvement (%)'].dropna(), bins=50)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Distribution of Performance Improvements (Segmented vs Normal)')\n",
    "plt.xlabel('Improvement (%)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(-100, 100)  # Focus on reasonable improvement range\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "improvement_stats = comparison_df['Improvement (%)'].describe()\n",
    "display(improvement_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance by Metric and k Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average improvement by metric type and k value\n",
    "metric_k_improvement = comparison_df.groupby(['Metric', 'k'])['Improvement (%)'].mean().reset_index()\n",
    "\n",
    "# Plot heatmap\n",
    "pivot_improvement = metric_k_improvement.pivot(index='Metric', columns='k', values='Improvement (%)')\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_improvement, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\n",
    "plt.title('Average Performance Improvement by Metric and k Value (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average improvement by model\n",
    "model_improvement = comparison_df.groupby('Base Model')['Improvement (%)'].mean().reset_index().sort_values('Improvement (%)')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=model_improvement, x='Base Model', y='Improvement (%)')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Average Performance Improvement by Model (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average improvement by dataset\n",
    "dataset_improvement = comparison_df.groupby('Dataset')['Improvement (%)'].mean().reset_index().sort_values('Improvement (%)')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=dataset_improvement, x='Dataset', y='Improvement (%)')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Average Performance Improvement by Dataset (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Comparison for Specific Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot metric comparison for a specific model and dataset\n",
    "def plot_model_comparison(base_model, dataset, metric):\n",
    "    filtered_df = comparison_df[(comparison_df['Base Model'] == base_model) & \n",
    "                               (comparison_df['Dataset'] == dataset) & \n",
    "                               (comparison_df['Metric'] == metric)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data found for {base_model} on {dataset} with metric {metric}\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot normal values\n",
    "    plt.plot(filtered_df['k'], filtered_df['Normal Value'], marker='o', label='Normal')\n",
    "    \n",
    "    # Plot each segmentation pattern if there are multiple\n",
    "    for pattern in filtered_df['Segmentation Pattern'].unique():\n",
    "        pattern_df = filtered_df[filtered_df['Segmentation Pattern'] == pattern]\n",
    "        label = f\"Segmented ({pattern})\" if pattern else \"Segmented\"\n",
    "        plt.plot(pattern_df['k'], pattern_df['Segmented Value'], marker='s', label=label)\n",
    "    \n",
    "    plt.title(f'{metric.upper()} at k: {base_model} on {dataset}')\n",
    "    plt.xlabel('k value')\n",
    "    plt.ylabel(f'{metric.upper()} value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique model-dataset combinations\n",
    "model_dataset_pairs = comparison_df[['Base Model', 'Dataset']].drop_duplicates().values\n",
    "\n",
    "# Plot the first few combinations as examples\n",
    "for i, (model, dataset) in enumerate(model_dataset_pairs[:3]):\n",
    "    print(f\"\\n### {model} on {dataset}\\n\")\n",
    "    \n",
    "    for metric in metric_types:\n",
    "        plot_model_comparison(model, dataset, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Different Segmentation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have multiple segmentation patterns to compare\n",
    "segmentation_patterns = comparison_df['Segmentation Pattern'].dropna().unique()\n",
    "print(f\"Available segmentation patterns: {segmentation_patterns}\")\n",
    "\n",
    "if len(segmentation_patterns) > 1:\n",
    "    # Calculate average improvement by segmentation pattern\n",
    "    pattern_improvement = comparison_df.dropna(subset=['Segmentation Pattern']).groupby('Segmentation Pattern')['Improvement (%)'].mean().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=pattern_improvement, x='Segmentation Pattern', y='Improvement (%)')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Average Performance Improvement by Segmentation Pattern (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance of Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Paired t-test to check if improvements are statistically significant\n",
    "t_stat, p_value = stats.ttest_rel(comparison_df['Segmented Value'], comparison_df['Normal Value'])\n",
    "\n",
    "print(f\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"{'Statistically significant improvement' if p_value < 0.05 and t_stat > 0 else 'Not statistically significant'} at α=0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall percentage of cases where segmentation improves performance\n",
    "improvement_count = (comparison_df['Improvement (%)'] > 0).sum()\n",
    "total_count = len(comparison_df)\n",
    "improvement_percentage = improvement_count / total_count * 100\n",
    "\n",
    "print(f\"Segmentation improves performance in {improvement_percentage:.2f}% of cases ({improvement_count} out of {total_count})\")\n",
    "\n",
    "# Calculate average improvement across all metrics\n",
    "avg_improvement = comparison_df['Improvement (%)'].mean()\n",
    "print(f\"Average performance improvement: {avg_improvement:.2f}%\")\n",
    "\n",
    "# Identify the best and worst cases\n",
    "best_case = comparison_df.loc[comparison_df['Improvement (%)'].idxmax()]\n",
    "worst_case = comparison_df.loc[comparison_df['Improvement (%)'].idxmin()]\n",
    "\n",
    "print(\"\\nBest improvement case:\")\n",
    "print(f\"  Model: {best_case['Base Model']}\")\n",
    "print(f\"  Dataset: {best_case['Dataset']}\")\n",
    "print(f\"  Metric: {best_case['Metric']} at k={best_case['k']}\")\n",
    "print(f\"  Improvement: {best_case['Improvement (%)']:.2f}%\")\n",
    "\n",
    "print(\"\\nWorst case:\")\n",
    "print(f\"  Model: {worst_case['Base Model']}\")\n",
    "print(f\"  Dataset: {worst_case['Dataset']}\")\n",
    "print(f\"  Metric: {worst_case['Metric']} at k={worst_case['k']}\")\n",
    "print(f\"  Degradation: {worst_case['Improvement (%)']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis compares the performance of models using normal image input versus segmented image input. The key findings are:\n",
    "\n",
    "1. **Overall Impact**: [Fill in based on results - whether segmentation generally helps or hurts]\n",
    "2. **Dataset-Specific Effects**: [Fill in which datasets benefit most from segmentation]\n",
    "3. **Model-Specific Effects**: [Fill in which models benefit most from segmentation]\n",
    "4. **Metric-Specific Effects**: [Fill in which metrics show most improvement]\n",
    "5. **Segmentation Pattern Effects**: [Fill in how different segmentation patterns compare]\n",
    "\n",
    "These findings can help guide decisions about when to use image segmentation in document visual question answering tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
