{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison - ArXiv QA Test Results\n",
    "\n",
    "This notebook compares the performance of different models on the `vidore_arxivqa_test_subsampled` dataset by analyzing their evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for our plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the metrics files for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results directory\n",
    "results_dir = \"./results/\"\n",
    "\n",
    "# Find all arxivqa_test_subsampled_metrics.json files\n",
    "arxivqa_files = []\n",
    "for root, dirs, files in os.walk(results_dir):\n",
    "    for file in files:\n",
    "        if file == \"vidore_arxivqa_test_subsampled_metrics.json\":\n",
    "            arxivqa_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(arxivqa_files)} model result files for ArXiv QA test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract model name from file path\n",
    "def get_model_name(file_path):\n",
    "    # Extract the model directory name\n",
    "    parts = file_path.split('/')\n",
    "    model_dir = [p for p in parts if p != 'results' and 'vidore_arxivqa_test' not in p][-1]\n",
    "    return model_dir\n",
    "\n",
    "# Load all the metrics into a dictionary\n",
    "model_metrics = {}\n",
    "\n",
    "for file_path in arxivqa_files:\n",
    "    model_name = get_model_name(file_path)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    # Extract the metrics we want to compare\n",
    "    metrics = data['metrics']['vidore/arxivqa_test_subsampled']\n",
    "    model_metrics[model_name] = metrics\n",
    "    \n",
    "print(f\"Loaded metrics for {len(model_metrics)} models.\")\n",
    "print(\"Models:\")\n",
    "for model in model_metrics.keys():\n",
    "    print(f\"- {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and organize key metrics for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics we want to compare\n",
    "metric_types = ['ndcg', 'map', 'recall', 'precision']\n",
    "k_values = [1, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "# Create a pandas DataFrame to hold all metrics for easy comparison\n",
    "all_metrics = []\n",
    "\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    for metric_type in metric_types:\n",
    "        for k in k_values:\n",
    "            metric_key = f\"{metric_type}_at_{k}\"\n",
    "            if metric_key in metrics:\n",
    "                all_metrics.append({\n",
    "                    'Model': model_name,\n",
    "                    'Metric': metric_type,\n",
    "                    'k': k,\n",
    "                    'Value': metrics[metric_key]\n",
    "                })\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "metrics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot metrics by k value\n",
    "def plot_metric(df, metric_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Filter for the specific metric\n",
    "    metric_df = df[df['Metric'] == metric_name]\n",
    "    \n",
    "    # Create line plot\n",
    "    sns.lineplot(data=metric_df, x='k', y='Value', hue='Model', marker='o', markersize=8)\n",
    "    \n",
    "    plt.title(f'{metric_name.upper()} at k for different models')\n",
    "    plt.xlabel('k (cutoff)')\n",
    "    plt.ylabel(f'{metric_name.upper()} Value')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each metric type\n",
    "for metric in metric_types:\n",
    "    plot_metric(metrics_df, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance at specific cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a bar chart comparing all models at a specific k value\n",
    "def compare_at_k(df, k_value):\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Filter for the specific k value\n",
    "    k_df = df[df['k'] == k_value]\n",
    "    \n",
    "    # Create bar chart\n",
    "    ax = sns.barplot(data=k_df, x='Model', y='Value', hue='Metric')\n",
    "\n",
    "    plt.title(f'Metrics at k={k_value} for all models')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all metrics at k=10 and k=100\n",
    "compare_at_k(metrics_df, 10)\n",
    "compare_at_k(metrics_df, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap comparison of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table for each metric type\n",
    "def create_heatmap(df, metric_name):\n",
    "    # Filter the dataframe for the specific metric\n",
    "    filtered_df = df[df['Metric'] == metric_name]\n",
    "    \n",
    "    # Create pivot table with models as rows and k values as columns\n",
    "    pivot_df = filtered_df.pivot(index='Model', columns='k', values='Value')\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, len(pivot_df) * 0.8))\n",
    "    sns.heatmap(pivot_df, annot=True, cmap=\"YlGnBu\", fmt=\".4f\", linewidths=.5)\n",
    "    plt.title(f'{metric_name.upper()} across different k values')\n",
    "    plt.xlabel('k (cutoff)')\n",
    "    plt.ylabel('Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmaps for each metric type\n",
    "for metric in metric_types:\n",
    "    create_heatmap(metrics_df, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of the best performing model for each metric and k value\n",
    "summary = []\n",
    "\n",
    "for metric in metric_types:\n",
    "    for k in k_values:\n",
    "        filtered = metrics_df[(metrics_df['Metric'] == metric) & (metrics_df['k'] == k)]\n",
    "        best_model = filtered.loc[filtered['Value'].idxmax()]\n",
    "        summary.append({\n",
    "            'Metric': metric,\n",
    "            'k': k,\n",
    "            'Best Model': best_model['Model'],\n",
    "            'Best Value': best_model['Value']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each model is the best\n",
    "best_model_counts = summary_df['Best Model'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "best_model_counts.plot(kind='bar')\n",
    "plt.title('Number of times each model performs best')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the analysis above, we can draw the following conclusions:\n",
    "\n",
    "1. The model with the best overall performance appears to be ...\n",
    "2. For specific metrics like NDCG and Recall, ... model performs particularly well\n",
    "3. At lower k values (1-5), ... model tends to perform better\n",
    "4. At higher k values (50-100), ... model shows superior performance\n",
    "\n",
    "These insights can help guide model selection for the ArXiv QA task based on specific performance requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
